# -*- coding: utf-8 -*-
"""index.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BToyXycQFcKM9-Ju2DnHoPw_dXsFHkq6

# Part 1: Acquire Training Data
"""

import numpy as np
from matplotlib import pyplot as plt
import time
import sqlite3
import pandas as pd

# csv file
train_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true"
# read in csv file
train_df = pd.read_csv(train_url)
# display file
train_df

"""# Part 2: Make a Dataset"""

# import `stopwords` from `nltk.corpus` to remove stopwords from the columns
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
stop = stopwords.words('english')

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras import losses
from tensorflow.keras import utils
from tensorflow.keras.layers import TextVectorization

def make_dataset(df):
  """
  1. Changes text to lowercase
  2. Remove stopwords (words usually considered to be uninformative such as
  "the," "and," or "but") from the article `text` and `title`
  3. Construct and return a `tf.data.Dataset`

  Returns a `tf.data.Dataset` with 2 inputs and 1 ouput
  """
  # make columns with text to be all lowercase
  df['title'] = df['title'].apply(lambda x: x.lower() if isinstance(x, str) else x)
  df['text'] = df['text'].apply(lambda x: x.lower() if isinstance(x, str) else x)
  # remove stopwords
  df['title'] = df['title'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))
  df['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))
  # create a TensorFlow `Dataset` from our dataframe
  # used particularly in text classification problems

  # inputs: form of (`title` column, `text` column)

  my_data_set = tf.data.Dataset.from_tensor_slices(({"title": df[['title']],
                                                     "text": df[['text']]},
                                                     {"fake": df[['fake']]}))

  # batch the Dataset prior to returning it
  # batching causes the model to train on chunks of data rather than individual rows (which
  # can reduce accuracy, but also greatly increase speed of training)
  my_data_set = my_data_set.batch(100)

  return my_data_set

"""### Validation Data

Now, we will perform the train-test split. We will also take out a small validation set.
"""

train_data = make_dataset(train_df).shuffle(buffer_size = len(make_dataset(train_df)), reshuffle_each_iteration=False)

train_size = int(0.8*len(train_data))
train = train_data.take(train_size)

# split 20% of primary Dataset to use for validation
val_size   = int(0.2*len(train_data))
val   = train_data.skip(train_size).take(val_size)

test  = train_data.skip(train_size + val_size)

len(train), len(val), len(test)

"""### Base Rate

The **base rate** refers to the accuracy of a model that always makes the same guess (for example, such a model might always say "fake news!").

By examining the labels on the training set, the base rate for this dataset is...
"""

# Determine base rate for data set by examining labels on training set
# Base rate is accuracy of model that always makes the same guess
num_real = 0
num_fake = 0

# Iterate through all of the batches in the training dataset
for batch in train.take(-1):
  # Then iterate through all of the labels in the "fake" column of the training dataset
  for label in batch[1]["fake"].numpy():
    # Increase the count for "real" news if the label equals 0
    if label == 0:
      num_real += 1
    # Increase the count for "fake" news if the label equals 1
    elif label == 1:
      num_fake += 1

# Print number of real news articles
print(f"Number of real news articles: {num_real}")
# Print number of fake news articles
print(f"Number of fake news articles: {num_fake}")
# Print base rate
print(f"Base rate: {num_fake/(num_fake + num_real)}")

"""### Text Vectorization"""

import re
import string
from keras import layers, losses
from keras.layers import TextVectorization

#preparing a text vectorization layer for tf model
size_vocabulary = 2000

def standardization(input_data):
    lowercase = tf.strings.lower(input_data)
    no_punctuation = tf.strings.regex_replace(lowercase,
                                  '[%s]' % re.escape(string.punctuation),'')
    return no_punctuation

title_vectorize_layer = TextVectorization(
    standardize=standardization,
    max_tokens=size_vocabulary, # only consider this many words
    output_mode='int',
    output_sequence_length=500)

title_vectorize_layer.adapt(train.map(lambda x, y: x["title"]))

"""# Part 3: Create Models

**RESEARCH QUESTION:**

When detecting fake news, is it most effective to focus on only the title of the article, the full text of the article, or both?

#### General Visualization Function:
"""

from keras import utils

def vis_plots(model):
  plot = utils.plot_model(model, "output_filename.png",
                       show_shapes=True,
                       show_layer_names=True)
  return plot

def graph_plots(model):
  plt.plot(history1.history["accuracy"],label='training')
  plt.plot(history1.history["val_accuracy"],label='validation')
  plt.legend()

"""#### Inputs and Output:"""

title_input = tf.keras.Input(shape = (1),
                            name = "title",
                            dtype = "string")

text_input = tf.keras.Input(shape = (1),
                            name = "text",
                            dtype = "string")

"""## **Model 1: Article TITLE as Input**

"""

# pipeline for title
# layers for processing title
title_features = title_vectorize_layer(title_input)
title_features = layers.Embedding(size_vocabulary, 3, name = "embedding_title")(title_features)
title_features = layers.Dropout(0.2)(title_features)
title_features = layers.GlobalAveragePooling1D()(title_features)
title_features = layers.Dropout(0.2)(title_features)
title_features = layers.Dense(32, activation = "relu")(title_features)

# creating output layer
main1 = layers.concatenate([title_features], axis = 1)
main1 = layers.Dense(32, activation = "relu")(main1)
output1 = layers.Dense(2, name = "fake")(main1)

# creating model with input and output
model1 = tf.keras.Model(inputs = title_input,
                        outputs = output1)

# compile model
model1.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

# train model
history1 = model1.fit(train_data,
                      validation_data=val,
                      epochs = 20,
                      verbose = True)

model1.summary()

"""#### Model 1 Visualization:"""

vis_plots(model1)

graph_plots(model1)

"""#### **Model 1 Performance Analysis**"""



"""## **Model 2: Article TEXT as Input**

"""

# pipeline for text
# layers for processing text
text_features = title_vectorize_layer(text_input)
text_features = layers.Embedding(size_vocabulary, 3, name = "embedding_texr")(text_features)
text_features = layers.Dropout(0.2)(text_features)
text_features = layers.GlobalAveragePooling1D()(text_features)
text_features = layers.Dropout(0.2)(text_features)
text_features = layers.Dense(32, activation = "relu")(text_features)

# creating output layer
main2 = layers.concatenate([text_features], axis = 1)
main2 = layers.Dense(32, activation = "relu")(main2)
output2 = layers.Dense(2, name = "fake")(main2)

# creating model with input and output
model2 = tf.keras.Model(inputs = text_input,
                        outputs = output2)

# compile model
model2.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

# train model
history2 = model2.fit(train_data,
                      validation_data=val,
                      epochs = 20,
                      verbose = True)

model2.summary()

"""#### Model 2 Visualization:"""

vis_plots(model2)

graph_plots(model2)

"""#### **Model 2 Performance Analysis**"""



"""## **Model 3: Both Article TITLE and TEXT as Input**

Uses the same pipeline and layers for processing the title as well as the pipeline and layers for processing the text!
"""

# pipeline for title
# layers for processing title
title_features = title_vectorize_layer(title_input)
title_features = layers.Embedding(size_vocabulary, 3, name = "embedding_title")(title_features)
title_features = layers.Dropout(0.2)(title_features)
title_features = layers.GlobalAveragePooling1D()(title_features)
title_features = layers.Dropout(0.2)(title_features)
title_features = layers.Dense(32, activation = "relu")(title_features)

# pipeline for text
# layers for processing text
text_features = title_vectorize_layer(text_input)
text_features = layers.Embedding(size_vocabulary, 3, name = "embedding_text")(text_features)
text_features = layers.Dropout(0.2)(text_features)
text_features = layers.GlobalAveragePooling1D()(text_features)
text_features = layers.Dropout(0.2)(text_features)
text_features = layers.Dense(32, activation = "relu")(text_features)

# creating output layer
main3 = layers.concatenate([title_features, text_features], axis = 1)
main3 = layers.Dense(32, activation = "relu")(main3)
output3 = layers.Dense(2, name = "fake")(main3)

# creating model with input and output
model3 = tf.keras.Model(inputs = [title_input, text_input],
                        outputs = output3)

# compile model
model3.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'],
              run_eagerly=True)

# train model
history3 = model3.fit(train_data,
                      validation_data=val,
                      epochs = 20,
                      verbose = True)

model3.summary()

"""#### Model 3 Visualization:"""

vis_plots(model3)

graph_plots(model3)

"""#### **Model 3 Performance Analysis**"""



"""### **Comparing All Models**"""

plt.plot(history1.history["val_accuracy"], label = "validation_1")
plt.plot(history2.history["val_accuracy"], label = "validation_2")
plt.plot(history3.history["val_accuracy"], label = "validation_3")

plt.legend()

"""# **Part 4: Model Evaluation**

Download the following test data set:
"""

# csv file
test_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true"
# read in csv file
test_df = pd.read_csv(test_url)
# display file
test_df

test_data = make_dataset(test_df)

model3.evaluate(test_data)

"""Write eval here

# **Part 5: Embedding Visualization**
"""

import plotly.express as px
import plotly.io as pio
pio.templates.default = "plotly_white"
from plotly.io import write_html

from sklearn.decomposition import PCA

# Look at embedding created by model
weights = model3.get_layer('embedding_title').get_weights()[0] # get the weights from the
vocab = title_vectorize_layer.get_vocabulary() # get the vocabulary from o

# PCA reduces multi-dimensional embeddings to a visualizable number
pca = PCA(n_components=2)
weights = pca.fit_transform(weights)

embedding_df = pd.DataFrame({
  'word' : vocab,
  'x0' : weights[:,0],
  'x1' : weights[:,1]
})
embedding_df

embedding_fig = px.scatter(data_frame = embedding_df,
                           x = "x0",
                           y = "x1",
                           hover_name = "word")

embedding_fig.show()

write_html(embedding_fig, "embedding_viz.html")